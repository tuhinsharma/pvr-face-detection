{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuhinsharma/.virtualenvs/pvr-object-recognition-v3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "sys.path.append(os.getcwd().replace(\"/notebooks\",\"\"))\n",
    "from object_recognition_platform.src import inception_resnet_v1\n",
    "import tensorflow as tf\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=1, thickness=2):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness)\n",
    "    \n",
    "    \n",
    "def load_network(model_path):\n",
    "    sess = tf.Session()\n",
    "    images_pl = tf.placeholder(tf.float32, shape=[None, 160, 160, 3], name='input_image')\n",
    "    images_norm = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), images_pl)\n",
    "    train_mode = tf.placeholder(tf.bool)\n",
    "    age_logits, gender_logits, _ = inception_resnet_v1.inference(images_norm, keep_probability=0.8,\n",
    "                                                                 phase_train=train_mode,\n",
    "                                                                 weight_decay=1e-5)\n",
    "    gender = tf.argmax(tf.nn.softmax(gender_logits), 1)\n",
    "    age_ = tf.cast(tf.constant([i for i in range(0, 101)]), tf.float32)\n",
    "    age = tf.reduce_sum(tf.multiply(tf.nn.softmax(age_logits), age_), axis=1)\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"restore model!\")\n",
    "    else:\n",
    "        pass\n",
    "    return sess,age,gender,train_mode,images_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess, age, gender, train_mode,images_pl = load_network(\"./models\")\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "face_directory = \"../faces\"\n",
    "\n",
    "filenames = os.listdir(face_directory)\n",
    "\n",
    "known_face_names = [filename.split(\".\")[0].split(\"_\")[0] for filename in filenames]\n",
    "\n",
    "known_face_encodings = [face_recognition.face_encodings(face_recognition.load_image_file(os.path.join(os.getcwd(),face_directory,filename)))[0] for filename in filenames]\n",
    "\n",
    "img_size = 160\n",
    "\n",
    "match_thresh = 0.49\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "    img = frame\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    #rgb_frame = frame[:, :, ::-1]\n",
    "    \n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"../models/shape_predictor_68_face_landmarks.dat\")\n",
    "    fa = FaceAligner(predictor, desiredFaceWidth=160)\n",
    "    \n",
    "    \n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    img_h, img_w, _ = np.shape(rgb_frame)\n",
    "    \n",
    "    # Find all the faces and face enqcodings in the frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # detect faces using dlib detector\n",
    "    detected = detector(rgb_frame, 1)\n",
    "    faces = np.empty((1, img_size, img_size, 3))\n",
    "    \n",
    "#     print(\"detected : \",detected)\n",
    "#     print(\"face_locations : \",face_locations)\n",
    "\n",
    "    # Loop through each face in this frame of video\n",
    "    for d, face_encoding in zip(detected, face_encodings):\n",
    "\n",
    "        (top, right, bottom, left) = d.top(),d.right(),d.bottom(),d.left()\n",
    "        \n",
    "        faces[0, :, :, :] = fa.align(rgb_frame, gray, d)\n",
    "        ages,genders = sess.run([age, gender], feed_dict={images_pl: faces, train_mode: False})\n",
    "        \n",
    "                \n",
    "        # See if the face is a match for the known face(s)\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        xmatches = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "\n",
    "        #print(\"xmatches : \",xmatches)\n",
    "        #print(\"matches : \",matches)\n",
    "\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # If a match was found in known_face_encodings, just use the first one.\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            # name = known_face_names[first_match_index]\n",
    "            idxx = np.argmin(xmatches)\n",
    "            if xmatches[idxx] < match_thresh:\n",
    "                name = known_face_names[idxx]\n",
    "                \n",
    "        label = \"{}, {}, {}\".format(name,int(ages[0]), \"F\" if genders[0] == 0 else \"M\")\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, label, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "        \n",
    "        \n",
    "    input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_h, img_w, _ = np.shape(input_img)\n",
    "\n",
    "    # detect faces using dlib detector\n",
    "    detected = detector(input_img, 1)\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "\n",
    "    for i, d in enumerate(detected):\n",
    "        x1, y1, x2, y2 = d.left(), d.top(), d.right() + 1, d.bottom() + 1\n",
    "\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        faces[i, :, :, :] = fa.align(input_img, gray, detected[i])\n",
    "\n",
    "    if len(detected) > 0:\n",
    "        # predict ages and genders of the detected faces\n",
    "        ages,genders = sess.run([age, gender], feed_dict={images_pl: faces, train_mode: False})\n",
    "\n",
    "\n",
    "    # draw results\n",
    "    for i, d in enumerate(detected):\n",
    "        label = \"{}, {}\".format(int(ages[i]), \"F\" if genders[i] == 0 else \"M\")\n",
    "        draw_label(img, (d.left(), d.top()), label)\n",
    "\n",
    "    cv2.imshow(\"result\", img)\n",
    "\n",
    "    # Display the resulting image\n",
    "#     cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/savedmodel.ckpt\n",
      "restore model!\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from imutils.face_utils import FaceAligner\n",
    "from object_recognition_platform.src import inception_resnet_v1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_network(model_path):\n",
    "    sess = tf.Session()\n",
    "    images_pl = tf.placeholder(tf.float32, shape=[None, 160, 160, 3], name='input_image')\n",
    "    images_norm = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), images_pl)\n",
    "    train_mode = tf.placeholder(tf.bool)\n",
    "    age_logits, gender_logits, _ = inception_resnet_v1.inference(images_norm, keep_probability=0.8,\n",
    "                                                                 phase_train=train_mode,\n",
    "                                                                 weight_decay=1e-5)\n",
    "    gender = tf.argmax(tf.nn.softmax(gender_logits), 1)\n",
    "    age_ = tf.cast(tf.constant([i for i in range(0, 101)]), tf.float32)\n",
    "    age = tf.reduce_sum(tf.multiply(tf.nn.softmax(age_logits), age_), axis=1)\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"restore model!\")\n",
    "    else:\n",
    "        pass\n",
    "    return sess, age, gender, train_mode, images_pl\n",
    "\n",
    "\n",
    "sess, age, gender, train_mode,images_pl = load_network(\"../models\")\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "face_directory = \"../faces\"\n",
    "\n",
    "filenames = os.listdir(face_directory)\n",
    "\n",
    "known_face_names = [filename.split(\".\")[0].split(\"_\")[0] for filename in filenames]\n",
    "\n",
    "known_face_encodings = [face_recognition.face_encodings(face_recognition.load_image_file(os.path.join(os.getcwd(),face_directory,filename)))[0] for filename in filenames]\n",
    "\n",
    "img_size = 160\n",
    "\n",
    "match_thresh = 0.49\n",
    "\n",
    "ret, frame = video_capture.read()\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"../models/shape_predictor_68_face_landmarks.dat\")\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize(frame,sess, age, gender, train_mode,images_pl,detector,predictor,fa):\n",
    "    img = frame\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    img_h, img_w, _ = np.shape(rgb_frame)\n",
    "\n",
    "    # Find all the faces and face enqcodings in the frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # detect faces using dlib detector\n",
    "    detected = detector(rgb_frame, 1)\n",
    "    faces = np.empty((1, img_size, img_size, 3))\n",
    "\n",
    "    result_list = []\n",
    "    \n",
    "    for d, face_encoding in zip(detected, face_encodings):\n",
    "        \n",
    "        result = dict()\n",
    "\n",
    "        (top, right, bottom, left) = d.top(), d.right(), d.bottom(), d.left()\n",
    "        print((top, right, bottom, left))\n",
    "\n",
    "        faces[0, :, :, :] = fa.align(rgb_frame, gray, d)\n",
    "        ages, genders = sess.run([age, gender], feed_dict={images_pl: faces, train_mode: False})\n",
    "\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        xmatches = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "\n",
    "\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # If a match was found in known_face_encodings, just use the first one.\n",
    "        if True in matches:\n",
    "            idx = np.argmin(xmatches)\n",
    "            if xmatches[idx] < match_thresh:\n",
    "                name = known_face_names[idx]\n",
    "                \n",
    "        person_age = int(ages[0])\n",
    "        person_gender = \"F\" if genders[0] == 0 else \"M\"\n",
    "        person_bounding_box = (top, right, bottom, left)\n",
    "        person_bounding_box_area = (bottom-top)*(right-left)\n",
    "        \n",
    "        result['name'] = name\n",
    "        result['age'] = person_age\n",
    "        result['gender'] = person_gender\n",
    "        result['bounding_box']= person_bounding_box\n",
    "        result['bounding_box_area'] = person_bounding_box_area\n",
    "                \n",
    "        result_list.append(result)\n",
    "        \n",
    "    return result_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = cv2.imread(\"../data/sample.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 596, 193, 544)\n",
      "{'name': 'Unknown', 'age': 29, 'gender': 'F', 'bounding_box': (141, 596, 193, 544), 'bounding_box_area': 2704}\n",
      "(47, 262, 121, 187)\n",
      "{'name': 'Unknown', 'age': 35, 'gender': 'F', 'bounding_box': (47, 262, 121, 187), 'bounding_box_area': 5550}\n",
      "(108, 439, 170, 377)\n",
      "{'name': 'Unknown', 'age': 31, 'gender': 'F', 'bounding_box': (108, 439, 170, 377), 'bounding_box_area': 3844}\n"
     ]
    }
   ],
   "source": [
    "res = recognize(mm,sess, age, gender, train_mode,images_pl,detector,predictor,fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Unknown',\n",
       "  'age': 29,\n",
       "  'gender': 'F',\n",
       "  'bounding_box': (141, 596, 193, 544),\n",
       "  'bounding_box_area': 2704},\n",
       " {'name': 'Unknown',\n",
       "  'age': 35,\n",
       "  'gender': 'F',\n",
       "  'bounding_box': (47, 262, 121, 187),\n",
       "  'bounding_box_area': 5550},\n",
       " {'name': 'Unknown',\n",
       "  'age': 31,\n",
       "  'gender': 'F',\n",
       "  'bounding_box': (108, 439, 170, 377),\n",
       "  'bounding_box_area': 3844}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvr-object-recognition-v3",
   "language": "python",
   "name": "pvr-object-recognition-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
